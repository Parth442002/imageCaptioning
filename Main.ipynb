{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#ML Modules\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR=\"./Flickr8kDataset/\"\n",
    "WORKING_DIR=\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Loading the VGG16 Model\n",
    "model=VGG16()\n",
    "# Restructure the model\n",
    "model=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
    "# Summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a6910abab642e3b214c1a4af5b4182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 18:42:25.723380: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-02 18:42:25.793333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Extracting features from the image\n",
    "features={}\n",
    "directory=os.path.join(BASE_DIR,\"Images\")\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "  # Load the image from file\n",
    "  imagepath=directory+'/'+img_name\n",
    "  image=load_img(imagepath,target_size=(224,224))\n",
    "  # Convert Image pexels to numpy array\n",
    "  image=img_to_array(image)\n",
    "  # Reshape Data for model\n",
    "  image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n",
    "  #Preprocess for VGG\n",
    "  image=preprocess_input(image)\n",
    "  #extract Features\n",
    "  feature=model.predict(image,verbose=0)\n",
    "  #Image ID\n",
    "  image_id=img_name.split(\".\")[0]\n",
    "  #Store the feature in Features dict\n",
    "  features[image_id]=feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Features Permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing data in Pickle File\n",
    "pickle.dump(features,open(\"Flickr8kFeatures.pickle\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load features from pickle file\n",
    "with open(\"Flickr8kFeatures.pickle\",\"rb\") as f:\n",
    "  features=pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e39552f0e0443b8908ee2ded49c478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc., \n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq',\n",
       " 'startseq black dog and spotted dog are fighting endseq',\n",
       " 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n",
       " 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n",
       " 'startseq two dogs of different breeds looking at each other on the road endseq',\n",
       " 'startseq two dogs on pavement moving toward each other endseq']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8485"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#Incoder Model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 35, 256)      2172160     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096)         0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 35, 256)      0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          1048832     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 256)          525312      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256)          0           ['dense_3[0][0]',                \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          65792       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 8485)         2180645     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,992,741\n",
      "Trainable params: 5,992,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecc37305ac1642d884efed714530af8b911a75783899eef687a345756cdd16b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
